@article{stoeckel2021,
	author = {St√∂ckel, Andreas and Eliasmith, Chris},
	title = {Passive Nonlinear Dendritic Interactions as a Computational Resource in Spiking Neural Networks},
	journal = {Neural Computation},
	volume = {33},
	number = {1},
	pages = {96-128},
	year = {2021},
	doi = {10.1162/neco\_a\_01338},
	note ={PMID: 33080158},
	URL = {https://doi.org/10.1162/neco_a_01338},
	eprint = {https://doi.org/10.1162/neco_a_01338},
	abstract = { Nonlinear interactions in the dendritic tree play a key role in neural computation. Nevertheless, modeling frameworks aimed at the construction of large-scale, functional spiking neural networks, such as the Neural Engineering Framework, tend to assume a linear superposition of postsynaptic currents. In this letter, we present a series of extensions to the Neural Engineering Framework that facilitate the construction of networks incorporating Dale's principle and nonlinear conductance-based synapses. We apply these extensions to a two-compartment LIF neuron that can be seen as a simple model of passive dendritic computation. We show that it is possible to incorporate neuron models with input-dependent nonlinearities into the Neural Engineering Framework without compromising high-level function and that nonlinear postsynaptic currents can be systematically exploited to compute a wide variety of multivariate, band-limited functions, including the Euclidean norm, controlled shunting, and nonnegative multiplication. By avoiding an additional source of spike noise, the function approximation accuracy of a single layer of two-compartment LIF neurons is on a par with or even surpasses that of two-layer spiking neural networks up to a certain target function bandwidth. }
}

