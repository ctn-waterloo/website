@inproceedings{morcos2018,
    author      = {Morcos, Benjamin and Stewart, Terrence C and
                   Eliasmith, Chris and Kapre, Nachiket},
    title       = {Implementing NEF Neural Networks on Embedded FPGAs},
    booktitle   = {International Conference on Field-Programmable
                   Technology (FPT)},
    year        = {2018},
    eventdate   = {2018-12-11/2018-12-14},
    location    = {Naha, Okinawa, Japan},
    publisher   = {IEEE},
    pages       = {25-32},
    doi         = {10.1109/FPT.2018.00015},
    abstract    = {Low-power, high-speed neural networks are critical for providing deployable
                embedded AI applications at the edge. We describe an FPGA implementation of
                Neural Engineering Framework (NEF) networks with online learning that
                outperforms mobile GPU implementations
                by an order of magnitude or more. Specifically, we provide an embedded
                Python-capable PYNQ FPGA implementation supported with a High-Level Synthesis (HLS)
                workflow that allows sub-millisecond implementation of adaptive neural
                networks with low-latency, direct I/O access to the physical world. We tune
                the precision of the different intermediate variables in the code to achieve
                competitive absolute accuracy against slower and larger floating-point
                reference designs. The online learning component of the neural network
                exploits immediate feedback to adjust the network weights to best support a
                given arithmetic precision. As the space of possible design configurations of
                such networks is vast and is subject to a target accuracy constraint, we use
                the Hyperopt hyper-parameter tuning tool instead of manual search to find
                Pareto optimal designs. Specifically, we are able to generate the optimized
                designs in under 500 iterations of Vivado HLS before running the complete
                Vivado place-and-route phase on that subset. For neural network populations of
                64--4096 neurons and 1--8 representational dimensions our optimized FPGA
                implementation generated by Hyperopt has a speedup of 10--484$\times$ over a
                competing cuBLAS implementation on the Jetson TX1 GPU  while using
                2.4--9.5$\times$ less power. Our speedups are a result of HLS-specific
                reformulation (15$\times$ improvement), precision adaptation (4$\times$
                improvement), and low-latency direct I/O access (1000$\times$ improvement).},
    pdf         = {http://compneuro.uwaterloo.ca/files/publications/morcos.2018.pdf},
}
